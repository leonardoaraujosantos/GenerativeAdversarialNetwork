{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest BEGAN\n",
    "Simple example using vanilla Neural networks. On Began the main difference is that the discriminator is made with an AutoEncoder which aims to perform well on real samples and poorly on generated samples, while the generator aims to produce adversarial samples which the discriminator cannot differentiation from real images.\n",
    "\n",
    "On this example we will only see normal fully connected layers, refer to the examples for more serious stuff.\n",
    "\n",
    "### Advantages\n",
    "* SOTA for face generation (2017), generating faces up to 128x128\n",
    "* Don't need batchnorm or dropout to stabiize training\n",
    "* Automatically balance image diversity and quality (More quality means more collapse)\n",
    "* Gives a convergence measure (Loss on DCGANS are normally meanigless)\n",
    "\n",
    "### References\n",
    "* [Blog](https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/)\n",
    "* [Paper](https://arxiv.org/pdf/1703.10717.pdf)\n",
    "* [Code I basically did copy/paste](https://github.com/wiseodd/generative-models/tree/master/GAN/boundary_equilibrium_gan)\n",
    "* [Implementation with faces on Tensorflow](https://github.com/artcg/BEGAN)\n",
    "* [Implementation with faces on Pytorch](https://github.com/sunshineatnoon/Paper-Implementations/tree/master/BEGAN)\n",
    "* [Wasserstein metric](https://en.wikipedia.org/wiki/Wasserstein_metric)\n",
    "* [Pix2Pix with Began](https://github.com/taey16/pix2pixBEGAN.pytorch)\n",
    "\n",
    "### Datasets to play\n",
    "* [CelebA dataset](https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM)\n",
    "* [Pedestrians dataset](http://mmlab.ie.cuhk.edu.hk/projects/luoWTiccv2013DDN/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Include modules from other directories\n",
    "import sys\n",
    "sys.path.append('../tensorflow/')\n",
    "import model_util as util\n",
    "import models\n",
    "import anim_util as anim\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "\n",
    "\n",
    "mb_size = 32\n",
    "X_dim = 784\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "m = 5\n",
    "lam = 1e-3\n",
    "diversity_ratio = 0.5\n",
    "k_curr = 0\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model inputs\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "k = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BEGAN Model\n",
    "As mentioned before with the BEGAN models the discriminator is composed by an AutoEncoder which has it's own loss \"Dist - Reconstruction loss\", this is not the discriminator loss.\n",
    "![alt text](began_arch.png \"Began Architecture\")\n",
    "\n",
    "### Main Idea\n",
    "Matching the distributions of the reconstruction losses (For generated and real images) can be a suitable proxy for matching the data distributions.\n",
    "\n",
    "The real loss is then derived from the Wasserstein distance between the reconstruction losses of real and generated data.\n",
    "\n",
    "### Gamma Parameter\n",
    "BEGAN has a hyperparameter that is used to balance generation quality vs diversity. More quality means mode mode collpapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def G(z):\n",
    "    with tf.variable_scope(\"G\"):\n",
    "        G_h1 = tf.nn.relu(util.linear_std(z, h_dim, 'g1', stddev=0.002))\n",
    "        #G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "        #G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "        G_log_prob = util.linear_std(G_h1, X_dim, 'g2', stddev=0.002)\n",
    "        G_prob = tf.nn.sigmoid(G_log_prob)    \n",
    "        return G_prob\n",
    "\n",
    "\n",
    "def D(X, reuse=None):\n",
    "    with tf.variable_scope(\"D\") as scope:\n",
    "        if reuse == True:\n",
    "            scope.reuse_variables()\n",
    "        # Autoencoder\n",
    "        #D_h1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)\n",
    "        D_h1 = tf.nn.relu(util.linear_std(X, h_dim, 'g1', stddev=0.002))\n",
    "        #X_recon = tf.matmul(D_h1, D_W2) + D_b2\n",
    "        X_recon = util.linear_std(D_h1, X_dim, 'g2', stddev=0.002)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        return tf.reduce_mean(tf.reduce_sum((X - X_recon)**2, 1))\n",
    "\n",
    "\n",
    "# Generator\n",
    "G_sample = G(z)\n",
    "\n",
    "# Reconstruction Loss (Autoencoder loss)\n",
    "D_real = D(X)\n",
    "D_fake = D(G_sample, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get adversarial loss\n",
    "D_loss = D_real - k*D_fake\n",
    "G_loss = D_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get trainable parameters for Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars = tf.trainable_variables()\n",
    "theta_D = [v for v in vars if v.name.startswith('D/')]\n",
    "theta_G = [v for v in vars if v.name.startswith('G/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizers\n",
    "Notice that we need only 2 optimizers to do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Convergence measure: 240.0\n",
      "Iter-1; Convergence measure: 18.42\n",
      "Iter-2; Convergence measure: 17.01\n",
      "Iter-3; Convergence measure: 13.67\n",
      "Iter-4; Convergence measure: 15.91\n",
      "Iter-5; Convergence measure: 17.01\n",
      "Iter-6; Convergence measure: 18.05\n",
      "Iter-7; Convergence measure: 18.96\n",
      "Iter-8; Convergence measure: 15.04\n",
      "Iter-9; Convergence measure: 15.02\n",
      "Iter-10; Convergence measure: 16.53\n",
      "Iter-11; Convergence measure: 15.25\n",
      "Iter-12; Convergence measure: 15.61\n",
      "Iter-13; Convergence measure: 17.62\n",
      "Iter-14; Convergence measure: 14.63\n",
      "Iter-15; Convergence measure: 15.47\n",
      "Iter-16; Convergence measure: 14.8\n",
      "Iter-17; Convergence measure: 14.77\n",
      "Iter-18; Convergence measure: 18.14\n",
      "Iter-19; Convergence measure: 14.61\n",
      "Iter-20; Convergence measure: 14.81\n",
      "Iter-21; Convergence measure: 15.4\n",
      "Iter-22; Convergence measure: 17.67\n",
      "Iter-23; Convergence measure: 14.87\n",
      "Iter-24; Convergence measure: 14.61\n",
      "Iter-25; Convergence measure: 15.07\n",
      "Iter-26; Convergence measure: 14.66\n",
      "Iter-27; Convergence measure: 15.47\n",
      "Iter-28; Convergence measure: 15.6\n",
      "Iter-29; Convergence measure: 15.09\n",
      "Iter-30; Convergence measure: 16.49\n",
      "Iter-31; Convergence measure: 15.55\n",
      "Iter-32; Convergence measure: 15.13\n",
      "Iter-33; Convergence measure: 14.37\n",
      "Iter-34; Convergence measure: 14.15\n",
      "Iter-35; Convergence measure: 15.4\n",
      "Iter-36; Convergence measure: 13.87\n",
      "Iter-37; Convergence measure: 15.09\n",
      "Iter-38; Convergence measure: 14.21\n",
      "Iter-39; Convergence measure: 14.8\n",
      "Iter-40; Convergence measure: 15.64\n",
      "Iter-41; Convergence measure: 14.62\n",
      "Iter-42; Convergence measure: 15.57\n",
      "Iter-43; Convergence measure: 14.77\n",
      "Iter-44; Convergence measure: 15.04\n",
      "Iter-45; Convergence measure: 15.3\n",
      "Iter-46; Convergence measure: 15.69\n",
      "Iter-47; Convergence measure: 12.59\n",
      "Iter-48; Convergence measure: 13.13\n",
      "Iter-49; Convergence measure: 13.89\n",
      "Iter-50; Convergence measure: 13.45\n",
      "Iter-51; Convergence measure: 14.94\n",
      "Iter-52; Convergence measure: 13.48\n",
      "Iter-53; Convergence measure: 14.17\n",
      "Iter-54; Convergence measure: 14.01\n",
      "Iter-55; Convergence measure: 13.29\n",
      "Iter-56; Convergence measure: 11.45\n",
      "Iter-57; Convergence measure: 11.28\n",
      "Iter-58; Convergence measure: 11.4\n",
      "Iter-59; Convergence measure: 12.14\n",
      "Iter-60; Convergence measure: 13.16\n",
      "Iter-61; Convergence measure: 14.42\n",
      "Iter-62; Convergence measure: 14.2\n",
      "Iter-63; Convergence measure: 13.83\n",
      "Iter-64; Convergence measure: 12.69\n",
      "Iter-65; Convergence measure: 13.31\n",
      "Iter-66; Convergence measure: 12.92\n",
      "Iter-67; Convergence measure: 14.76\n",
      "Iter-68; Convergence measure: 12.41\n",
      "Iter-69; Convergence measure: 12.76\n",
      "Iter-70; Convergence measure: 13.47\n",
      "Iter-71; Convergence measure: 14.38\n",
      "Iter-72; Convergence measure: 12.81\n",
      "Iter-73; Convergence measure: 13.34\n",
      "Iter-74; Convergence measure: 15.58\n",
      "Iter-75; Convergence measure: 12.41\n",
      "Iter-76; Convergence measure: 13.51\n",
      "Iter-77; Convergence measure: 13.8\n",
      "Iter-78; Convergence measure: 14.94\n",
      "Iter-79; Convergence measure: 13.91\n",
      "Iter-80; Convergence measure: 14.56\n",
      "Iter-81; Convergence measure: 13.79\n",
      "Iter-82; Convergence measure: 13.16\n",
      "Iter-83; Convergence measure: 13.55\n",
      "Iter-84; Convergence measure: 13.81\n",
      "Iter-85; Convergence measure: 12.34\n",
      "Iter-86; Convergence measure: 11.55\n",
      "Iter-87; Convergence measure: 13.73\n",
      "Iter-88; Convergence measure: 14.26\n",
      "Iter-89; Convergence measure: 12.33\n",
      "Iter-90; Convergence measure: 13.87\n",
      "Iter-91; Convergence measure: 15.02\n",
      "Iter-92; Convergence measure: 11.8\n",
      "Iter-93; Convergence measure: 12.65\n",
      "Iter-94; Convergence measure: 12.03\n",
      "Iter-95; Convergence measure: 13.51\n",
      "Iter-96; Convergence measure: 13.63\n",
      "Iter-97; Convergence measure: 14.26\n",
      "Iter-98; Convergence measure: 10.87\n",
      "Iter-99; Convergence measure: 12.78\n"
     ]
    }
   ],
   "source": [
    "for it in range(100000):\n",
    "    # Get batch of images\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    # Optimize Discriminator\n",
    "    _, D_real_curr = sess.run(\n",
    "        [D_solver, D_real],\n",
    "        feed_dict={X: X_mb, z: sample_z(mb_size, z_dim), k: k_curr}\n",
    "    )\n",
    "\n",
    "    # Optimize Generator\n",
    "    _, D_fake_curr = sess.run(\n",
    "        [G_solver, D_fake],\n",
    "        feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)}\n",
    "    )\n",
    "\n",
    "    # Adaptive term to balance \n",
    "    k_curr = k_curr + lam * (diversity_ratio*D_real_curr - D_fake_curr)\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        measure = D_real_curr + np.abs(diversity_ratio*D_real_curr - D_fake_curr)\n",
    "\n",
    "        print('Iter-{}; Convergence measure: {:.4}'.format(it//1000, measure))\n",
    "\n",
    "        samples = sess.run(G_sample, feed_dict={z: sample_z(16, z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(it//1000).zfill(3)), bbox_inches='tight')        \n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
